Суммирование элементов массива
=========

Тестирование проводилось на **NVIDIA RTX 4090**, т.е. **1008 GB/s** пропускная способность памяти и **73.1 TFlops**
(т.е. миллиардов floating-point операций в секунду). Спецификацию удобно смотреть
в [табличках подобных этой](https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units).

Рассматриваемые варианты реализации кернела:

| Реализация                   | Описание                                                                             | Достигнутая скорость обработки | Утилизация ПСП (Speed Of Light - **SOL**) |
|------------------------------|--------------------------------------------------------------------------------------|--------------------------------|-------------------------------------------|
| sum_1                        | Добавление в аккумулятор (race condition)                                            | ~~318 GB/s~~ (race condition)  | 32%                                       |
| sum_2                        | Добавление через ```atomic_add``` в аккумулятор                                      | 241 GB/s                       | 24%                                       |
| sum_3 n=10^7                 | 64 элемента подряд + ```atomic_add``` в аккумулятор                                  | 422 GB/s                       | 42%                                       |
| sum_3 n=10^8                 | 64 элемента подряд + ```atomic_add``` в аккумулятор                                  | 392 GB/s                       | 39%                                       |
| sum_3                        | 64 элемента подряд (кэш) + ```atomic_add``` в аккумулятор + поправить размер NDRange | 638 GB/s                       | 63%                                       |
| sum_3_no_cache_but_coalesced | 64 элемента c разрывом + **coalesced**     + ```atomic_add``` в аккумулятор          | 865 GB/s                       | 86%                                       |
| sum_3_no_cache_no_coalesced  | 64 элемента c разрывом + **NON coalesced** + ```atomic_add``` в аккумулятор          | 32 GB/s                        | 3%                                        |

```Упражнение``` Почему в табличке есть **Достигнутая скорость обработки**? Чем удобна такая метрика? Это то число что мы выводим в консоль, т.е. оценка объема данных входного массива который был бы обработан в секунду.

```Упражнение``` Отличается ли эта метрика чем-то от достигнутой **ПСП**? (Пропускной Способности Памяти)

Давайте набагаем
=========

Давайте сделаем умышленную багу - откройте ```.cpp``` файл и сместите инициализацию аккумулятора для суммы (т.е. ее зануление) - пусть теперь эта операция делается не перед каждым запуском кернела, а вне цикла, сразу после аллокации 4-байтного буфера под этот аккумулятор:

```c++
gpu::gpu_mem_32i sum_gpu;
const int sum_is_zero = 0;
sum_gpu.resizeN(1);
sum_gpu.writeN(&sum_is_zero, 1); // эту строку мы перенесли сюда из for (unsigned int i = 0; i < niters; ++i) { ... }
```

Теперь давайте зажмуримся, стукнемся лбом о подушку (сильно) и забудем где и как мы набагали.

Давайте отладим
=========

Итак, теперь при запуске мы получаем подобную ошибку:

```
GPU results should be equal to CPU results! But 2111865288 != 1070179988
```

Как это отладить? Для начала давайте упростим проблему максимально, чтобы можно было уместить все данные в голове.

Проверяем - воспроизводится ли проблема если создать массив размера ```n=1``` вместо ```n=10*1000*1000``` (изменяется в ```.cpp``` файле).

Да, проблема тоже наблюдается:

```
GPU results should be equal to CPU results! But 1769119550 != 176911955
```

Итак мы знаем что с точки зрения ЦПУ сумма равна 176911955, а так как число одно, то видимо это и есть это единственное число. Проверяем под отладчиком - действительно ```as[0]``` равно этому числу.

Давайте посмотрим как это выглядит со стороны видеокарты. Один из способов - вывести в консоль значения переменных которые видит видеокарта через ```printf```:

```c++
#ifdef __NVCC__ // если это CUDA а не OpenCL (nvcc - компилятор CUDA)
    printf("before atomic add index=%d n=%d sum=%d a[index]=%d\n", index, n, sum[0], a[index]);
#endif

    atomic_add(sum, a[index]);

#ifdef __NVCC__
    printf("after  atomic add index=%d n=%d sum=%d\n", index, n, sum[0]);
#endif
```

Получаем вывод:

```
Data generated for n=1!
before atomic add index=0 n=1 sum=0 a[index]=176911955
after  atomic add index=0 n=1 sum=176911955
before atomic add index=0 n=1 sum=176911955 a[index]=176911955
after  atomic add index=0 n=1 sum=353823910
...
after  atomic add index=0 n=1 sum=1592207595
before atomic add index=0 n=1 sum=1592207595 a[index]=176911955
after  atomic add index=0 n=1 sum=1769119550
GPU: 6.9e-05+-1.1547e-06 s
Reached memory bandwidth: 5.39897e-05 GB/s
GPU results should be equal to CPU results! But 1769119550 != 176911955
```

Почему строчек из ```printf``` не две а 20? Потому что мы 10 раз запускаем каждый кернел.

Почему результат не совпал (```1769119550 != 176911955```) но при этом во второй строке вывода мы видим что как раз ```sum``` обладает правильным значением? ```before atomic add index=0 n=1 sum=176911955 a[index]=176911955```

Правильный ответ - при отладке баги стоит выключить мозг и стараться технично исследовать что же в принципе произошло и почему. Тогда будет ясно где бага и вы не будете скатываться в домыслы, которые основываются на том "что вы хотели закодить" вместо "что же я на самом деле накодил". Представьте что вы робот для отладки который не знает никакой истории написания кода, представьте что код писали не вы вовсе.

В таком случае финальная упавшая проверка должна (чисто техничеси) стыковаться с последним выводом ```printf```. А какое там значение было заявлено ```sum```? Как раз то же что и в упавшей проверке: ```after  atomic add index=0 n=1 sum=1769119550```.

Смотрим на остальные строки - видим что почему то вывод меняется от запуска к запуску кернела. Как такое может быть? Давайте подумаем что определяет результат работы вывода (это размышления близкие к тому чтобы сформулировать "что делает мой подозрительный кусочек кода не чисто без-контекстно функциональным"):

- Исходники кернела - но они неизменны от запуска к запуску
- Входные данные - но они неизменны от запуска к запуску (**как доказать себе это технически в коде?**)
- Пока мы себе **чисто технически** доказываем про неизменность входных данных - обнаруживаем что жизнь ```sum_gpu``` ассимитрична - этот буфер инициализируется нулем лишь перед первым запуском

Вот и нашли баг. Проверяем что его исправление все спасает.

Не забываем вернуть ```n=10*1000*1000``` и проверить что при нем тоже все теперь работает хорошо.

```Упражнение``` А что делать если бы проблема воспроизводилась только на большом массиве?

```Упражнение``` Как сделать так чтобы ```printf``` не заспамил весь терминал (ведь он вызывается для каждого **workitem**)?

Давайте отпрофилируем sum_3
=========

Итак взглянем на вариант ```sum_3``` в котором делается суммирование 64 элементов подряд + ```atomic_add``` этой суммы в общий аккумулятор.

Мы видим в результатах всего **422 GB/s = 42% SOL**, т.е. явно можно быстрее.

```Упражнение``` Подумайте, в чем неоптимальность такого кернела? Как можно сделать его быстрее?

Но давайте не думать а полагаться на чисто-техническую автоматическую аналитику - на профилировщик.

Запускаем профилировщик:

```
sudo /usr/local/cuda/nsight-compute-2023.1.1/ncu-ui
```

Жмем ```New Project```:

- ```Application Executable``` копируем путь к бинарнику из окна запуска ```Run``` в ```CLion``` - например ```/home/polarnick/coding/forks/GPGPUTasks2023/cmake-build-relwithdebinfo/main52_sum```
- ```Output File``` создаем какой-нибудь файл ```profileout``` в какой-нибудь ```tmp``` папке
- В ```Metrics``` проверяем что выставлена галка ```full```

Смотрим на результаты:

![NVIDIA Nsight Compute results of sum profiling](/docs/images/nvc_sum3_summary.png?raw=true)

Видим что достигнуто ```63%``` **ПСП**, что не очень стыкуется с ```42% SOL```.

```Упражнение``` Почему так?

Кликаем на **12 Issues Detected**. Меня очень заинтересовало что нам явно говорят про **uncoalesced memory global memory access**:

![NVIDIA Nsight Compute uncoalesced memory global memory access](/docs/images/nvc_sum3_uncoalesced.png?raw=true)

```Упражнение``` Если все так плохо - то не вступает ли это в противоречие с другими нашими наблюдениями?

```Упражнение``` Давайте посмотрим на код ```sum_3``` и попробуем себе доказать что доступ к видеопамяти является **coalesced**.

```Упражнение``` Если доступ **uncoalesced** - то почему у нас такой высокий результат **ПСП**?

```Упражнение``` Как найти ответы на предыдущий вопрос чисто технически не включая мозг? Как прокинуть мостик от факта ```uncoalesced``` до факта ```общая ПСП высокая```?

Чтобы прокинуть мостик от факта ```uncoalesced``` до факта ```общая ПСП высокая``` можно взглянуть на общую картину где сколько и с какой скоростью данных отправляется - разворачиваем ```Memory Workload Analysis```:

![NVIDIA Nsight Compute memory workload analysis](/docs/images/nvc_sum3_uncoalesced_memory_stats.png?raw=true)

И видим что возможно нас спас большой L2-кэш и маленький объем данных.

```Упражнение``` Как узнать какой у нас объем L2-кэша? Как посчитать какой объем данных?

```Упражнение``` Как окончательно проверить эту гипотезу?

```Упражнение``` Как сломать это везение чтобы огрести замедление от ```uncoalesced``` паттерна доступа?

Действительно, ```n=10*1000*1000``` это всего лишь **40 мегабайт данных**, при этом у RTX 4090 (см. **L2 Cache** [тут](https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units)) целых **72 мегабайта** кэша!

Давайте сломаем это увеличив объем данных в 10 раз чтобы не влезть в кэш - ```n=100*1000*1000``` (теперь их **400 мегабайт**).

Получили **392 GB/s ПСП = 39% SOL**: ```Reached memory bandwidth: 392.387 GB/s```.

А если мы вспомним что размер рабочей нагрузки на самом деле теперь не ```n``` а что-то вроде ```n/64``` (т.е. если мы исправим размер NDRange - уменьшив его), то результат и вовсе улучшится:

Получим **638 GB/s ПСП = 63% SOL**!

Эти результаты подозрительно неплохи, и если посмотреть под профилировщиком то мы увидим что опять кэш-хит хорош. На мой взгляд так происходит потому что рабочие потоки запускаются подряд, поэтому данные обрабатываются тоже в целом подряд, а значит срабатывает кэширование локального подрегиона данных.

```Упражнение``` Как проверить эту гипотезу?

```Упражнение``` Как сломать и усугибить этот паттерн? Как сделать так чтобы кэш перестал спасать?

Давайте из любопытства сделаем так чтобы в кэш перестало попадать так много данных, т.е. вместо варианта когда все подгрузки рядом:

![Cached version](/docs/images/nvc_sum3_cached.png?raw=true)

Давайте подгружать с большим перешагом, так что элементы для одного **workitem** оказываются равномерно размазаны по всему массиву:

![Uncached version](/docs/images/nvc_sum3_overflown_cache.png?raw=true)

Получаем **865 GB/s ПСП = 86% SOL**!

```Упражнение``` Почему так? Ведь мы должны были навредить использованию кэша и явно проиграть из-за этого?

```Упражнение``` Как проверить навредили ли мы кэшу?

Давайте посмотрим из-под профилировщика - навредили ли мы кэшу, снизился ли **cache-hit**:

![NVIDIA Nsight Compute low cache-hit](/docs/images/nvc_sum3_overflown_cache_profile.png?raw=true)

Действительно, теперь **cache-hit** крайне низкий.

```Упражнение``` Но почему тогда доступ к видеопамяти такой быстрый?

```Упражнение``` Какие основные требования всегда были к паттерну доступа чтобы обращение к видеопамяти было быстрым?

```Упражнение``` Давайте подумаем - обладает ли наш паттерн доступа coalesced свойством?

Казалось бы действительно нас спасло **coalesced** свойство нашего алгоритма, которое появилось пока мы пытались сломать обращения к кэшу. Вспомним как выглядит паттерн доступа:

![Uncached version](/docs/images/nvc_sum3_overflown_cache.png?raw=true)

```Упражнение``` Как в дополнение к уже сломанному **cache-hit** сломать еще и **coalesced** паттерн?

Чтобы сломать паттерн - надо сделать так чтобы **work-item**-ы перемешались между собой, т.е. чтобы стартовый индекс был у них случайный.

Поэтому давайте явно создадим в ```.cpp``` файле массив ```shuffled_indices``` и прогрузим его на ГПУ, создав соответствующую модификацию кернела:

```c++
void sum_3_no_cache_no_coalesced(__global const int* a,
                                 __global const int* shuffled_indices, // нам передали случайную перестановку индексов
                                 __global       int* sum,
                                       unsigned int  n) {
    const size_t index = get_global_id(0);

    const size_t step_size = (n + VALUES_PER_WORKITEM - 1) / VALUES_PER_WORKITEM;
    if (index >= step_size)
        return;

    size_t random_index = shuffled_indices[index]; // берем доставшийся нам стартовый индекс

    int mySum = 0;
    for (int i = 0; i < VALUES_PER_WORKITEM; ++i) {
        mySum += a[random_index + i * step_size];
    }

    atomic_add(sum, mySum);
}
```

```Упражнение``` Перед запуском подумайте и загадайте - какой процент **ПСП**/**SOL** вы бы ожидали получить?

Не забываем в ```.cpp``` файле заменить имя вызываемого кернела и запускаем.

Получаем **31 GB/s ПСП = 3% SOL**: ```Reached memory bandwidth: 30.8956 GB/s```

Ура, наконец-то мы все сломали! И действительно, мы теперь получили максимально **не-coalesced** доступ, а значит в каждой запрошенной кэш-линии должна была использоваться лишь ```1/32``` доля полученных данных! Так что полученное замедление в 29 раз выглядит ожидаемым.

Давайте теперь посмотрим на все три профиля в профилировщике:

- алгоритма где нет **coalesced** но зато спасает кэш (```sum_3```)
- алгоритма где не спасает кэш но спасает **coalesced** (```sum_3_no_cache_but_coalesced```)
- алгоритма где не спасает кэш и нет **coalesced** благодаря перестановке индексов (```sum_3_no_cache_no_coalesced```)

1) Запускаем один вариант и указываем какой-то ```Output File```
2) Запускаем другой вариант и указываем какой-то другой ```Output File```
3) Запускаем третий вариант и указываем какой-то другой ```Output File```
4) Теперь у нас три вкладки с тремя результатами профилирования, давайте их сравним
5) Для этого возьмем второй вариант (где не спасает кэш но спасает **coalesced** - ```sum_3_no_cache_but_coalesced```) и сделаем его точкой отсчета (```Add Baseline```) - т.е. будем сравнивать первый и третий вариант относительно второго:

![NVIDIA Nsight Compute how to set baseline profile](/docs/images/nvc_sum3_add_baseline.png?raw=true)

6) Теперь откроем первый профиль (где нет **coalesced** но спасает кэш) - среди прочего видим картину драматически большего кэш-хита например:

![NVIDIA Nsight Compute comparison of cache hit](/docs/images/nvc_sum3_diff_cache_hit.png?raw=true)

7) А как выглядит третий профиль (где нет **coalesced** и нет кэша) относительно второго варианта (где есть **coalesced** но нет кэша):

![NVIDIA Nsight Compute comparison of large memory transfer due to non-coalesced memory access pattern](/docs/images/nvc_sum3_diff_large_transfer_non_coalesced.png?raw=true)

Т.е. объем данных который действительно пришлось прогрузить из видеопамяти оказался на 658% больше, т.е. в x7.5 раз больше.
