Пусть у нас в видеопамяти **N картинок** и нам нужно что-то посчитать между каждой парой, т.е. например нам надо взять первую картинку и попиксельно оценить ее похожесть с каждой другой картинкой:

![each image with each](/docs/images/111_images_each_with_each.png?raw=true)

Итого нам нужно выполнить ```O(N^2)``` вычислительных задач - посчитать похожесть двух картинок. Это много вычислений, гораздо больше чем мы должны для этого прогрузить ```CPU -> GPU```. Так что пока что все хорошо.

Но представим что нам не повезло и результаты этих рассчетов - ```O(N^2)``` картинок нам надо прогрузить обратно ```GPU -> CPU```.

Допустим мы были аккуратны и при разработке добавили логгирование распроцентовок навроде:

```
processing done in 1234 s = 20% IO + 10% CPU + 5% upload to VRAM + 30% GPU + 35% read from VRAM
```

И видим что последний этап чтения с видеокарты занимает подозрительно много. Для драматизма - пусть мы там увидели 80%. Это значит что если мы его ускорим то мы в сумме можем получить ускорение до х5 раз! 

Отлично, смотрим на картину мира чуть с менее высокой точки зрения - через профилировщик смотрим на timeline - видим там все то же - **много пузырей чтения данных GPU -> CPU**.

```Упражнение``` как "взвесить килограммами"? Как проверить ожидаемый ли это случай? У нас есть время обработки достигнутое на практике, а как оценить время обработки которое мы бы ожидали получить?

Давайте взвесим килограммами - раз у нас узкое место считывание данных по **PCI-E** шине - то оценим объем данных которые мы собираемся считывать (в гигабайтах) - можно даже в коде прямо посчитать и вывести в логе.

А затем, взяв ПСП **PCI-E** шины за **8 GB/s** рассчитать ожидаемое время отправки данных.

**Внезапно** оказывается что ожидаемое время отправки результатов значительно (х10) медленнее чем должно быть. И под профилировщиком эта оценка подтверждается - вместо условных **8 GB/s** мы видим **500 MB/s**.

```Упражнение``` какой кусок кода за это может быть виновен? Где искать?

Раз мы четко знаем кто виноват - операция трансфера данных ```VRAM -> CPU``` то это очень маленький кусок кода вокруг Vulkan API вызовов трансфера данных, и среди этого кода обнаруживается временный буфер в **RAM** который аллоцирован через **Vulkan API**. И у аллокации этого буфера есть множество настроек, в т.ч. указание "в какую сторону ожидается трансфер данных", и это указание оказывается ошибочно выставленным в ```CPU -> GPU```.